# -*- coding: utf-8 -*-
"""NN2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1la4GpsjmrmGnlIV_VvdAO-eX1M2J5pik
"""

import pandas as pd
url="https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv"
df=pd.read_csv(url)
df.head()

"""This is an ex code to showcase NN in production"""

!pip install keras-tuner

import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pickle
import tensorflow as tf
from tensorflow.keras import layers, models
import keras_tuner as kt

"""pd.set_option("display.max_columns",None)#show u all the columns
pd.set_option("display.expand_frame_repr",False)
pd.set_option("max_colwidth",None)
#this allows to show all columns
"""

df.head()

df.info()

df["TotalCharges"]=pd.to_numeric(df["TotalCharges"],errors='coerce')
df["gender"]=df["gender"].map({"Female":0,"Male":1})
df["Churn"]=df["Churn"].map({"No":0,"Yes":1})
df.head()

df.isna().sum()

#drop
df.dropna(inplace=True)

#this is only for factorise function
lab,val=pd.factorize(['yes','no','yes','no','maybe','maybe','no'])#unique value
print(lab)
print(val)

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
import pandas as pd

# factorize categorical columns
numerical_features = [
    'gender','SeniorCitizen','tenure','OnlineBackup',
    'TechSupport','MonthlyCharges','Dependents',
    'TotalCharges','PaperlessBilling','PaymentMethod'
]

for col in numerical_features:
    if df[col].dtype == "object":
        df[col] = pd.factorize(df[col])[0]

# validation step
numerical_features = [f for f in numerical_features if f in df.columns]
X = df[numerical_features]
y = df["Churn"]

# Random Forest with RFE
rf = RandomForestClassifier(n_estimators=120, random_state=42)
rfe = RFE(estimator=rf, n_features_to_select=5)
#n_features_to_select=5 in RFE means:
#Out of all your input features (here ~10),
#RFE will keep only the best 5 features,
rfe.fit(X, y)

# Selected features
selected_features = [feature for feature, selected in zip(numerical_features, rfe.support_) if selected]
#creates list [('gender', False), ('SeniorCitizen', True), ('tenure', True), ...]
#as 5 featues are selected ,here in zip opens the list,if selected->true add this features
print(f"Selected features by RFE: {selected_features}")

# Feature importance
rf.fit(X, y)
importances = pd.DataFrame({'features': numerical_features,
                            'importance': rf.feature_importances_})
importances = importances.sort_values(by='importance', ascending=False)

print(importances)

import seaborn as sns
plt.figure(figsize=(8,8))
sns.barplot(x='importance',y='features',data=importances)
plt.title("Features importants")
plt.show()

#start build
x=df[selected_features]
y=df["Churn"]
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=12)
scaler=StandardScaler()
#this is used because in colume for large value dominates creates error ther fore it create a value range of -1 to 1
x_train_scaled=scaler.fit_transform(x_train)
x_test_scaled=scaler.transform(x_test)
#to save data of all above code ,save it in pkl(pickle)
with open("scaler.pkl","wb") as file:
    pickle.dump(scaler,file)

def create_model(hp=None):
  if hp: #initially hyperparameter is none it can be chaged
    model=models.Sequential()
    #firt layer
    model.add(layers.Dense(
        units=hp.Choice('units_1',[8,16,32,64]),
        activation='relu',
        input_shape=(x_train_scaled.shape[1],)
    ))

    #optional second layer
    if hp.Boolean('second_layer'):
      model.add(layers.Dense(
          units=hp.Choice('units_2',[8,16,32]),
          activation='relu'
      ))

    #output layer
    model.add(layers.Dense(1,activation='sigmoid'))
    model.compile(
        optimizer="adam",
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )


    #values means it gives no. of nodes it chooses it randomly
    #hp.boolen means if it is true second layer will be there else not ,it is also randomly selected
  else:
    model=models.Sequential([layers.Dense(64,activation='relu',input_shape=(x_train_scaled.shape[1],)),layers.Dense(64,activation='relu'),layers.Dense(32,activation='relu'),layers.Dense(16,activation='relu'),layers.Dense(8,activation='relu'),layers.Dense(1,activation='sigmoid')])
    #.shape[1] u can put 0 or other not  all because x_train_scaled is 2d array we need size just as len(mat[0])
    #4 hiddin layer and last is out putnode(0/1)
    model.compile(optimizer="adam",loss="binary_crossentropy",metrics=["accuracy"])
  return model
#build
innitial_model=create_model()#hyperparametre is not defined so else statment is running
innitial_model.summary()
histry=innitial_model.fit(x_train_scaled,y_train,epochs=50,batch_size=32,validation_split=0.2,verbose=1)

loss, accuracy = innitial_model.evaluate(x_test_scaled, y_test)
print(f"loss:{loss},accuracy:{accuracy}")

plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.plot(histry.history['loss'],label='trainloss')
plt.plot(histry.history['val_loss'],label='valloss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title("model loss curve")
plt.grid(alpha=0.3)

plt.figure(figsize=(12,6))
plt.subplot(1,2,2)
plt.plot(histry.history['accuracy'],label='traning accuracy')
plt.plot(histry.history['val_accuracy'],label='validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('accuracy')
plt.legend()
plt.title("model accuracy curve")
plt.grid(alpha=0.3)

#hyper paprameter tuning
tuner=kt.RandomSearch(
    create_model,#model name
    objective='val_accuracy',#monitor through val_accuracy
    max_trials=5,#max 5 trials
    directory = 'kerus_tuner',#save my all trial prediction
    project_name='churn_pred'
)
#if val_loss is same for repeated 5 trial trial stop the code
stop_early=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5)
tuner.search(x_train_scaled,y_train,epochs=50,validation_split=0.2,callbacks=[stop_early],verbose=1)
best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]
print("Best hyperparameters found")
for param in best_hps.values:
  print(f"{param}:{best_hps.get(param)}")

best_model=tuner.hypermodel.build(best_hps)
best_history=best_model.fit(x_train_scaled,y_train,epochs=30,batch_size=32,validation_split=0.2,callbacks=[stop_early],verbose=1)
best_loss,best_accuracy=best_model.evaluate(x_test_scaled,y_test)
print(f"Tuned Model -Test Loss:{best_loss}")
print(f"Tuned Model -Test Accuracy:{best_accuracy}")
plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.plot(best_history.history['loss'],label='trainloss')
plt.plot(best_history.history['val_loss'],label='valloss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Tuned Model -Loss Curve')
plt.legend()
plt.show()

plt.subplot(1,2,2)
plt.plot(best_history.history['accuracy'],label='trainaccuracy')
plt.plot(best_history.history['val_accuracy'],label='valaccuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Tuned Model -Accuracy Curve')
plt.legend()
plt.show()

print("\nModel comparision")
print(f"Initial Model Accuracy: {accuracy:.4f}")
print(f"Tuned Model Accuracy: {best_accuracy:.4f}")
print(f"Improment:{(best_accuracy-accuracy)*100:.2f}%")

#save the best model
best_model.save('best_model.h5')#save the model
#best_model.save('best_model_new.keras')
with open('selected_features.pkl','wb') as file:#save the selected
  pickle.dump(selected_features,file)

#how to use data which is stored
def load_model_component():
  #load model in h5
  model=tf.keras.models.load_model('best_model.h5')

  with open('scaler.pkl','rb') as file:#rb-read binary
    scaler=pickle.load(file)

  with open('selected_features.pkl','rb') as file:
    selected_features=pickle.load(file)

  return model,scaler,selected_features

#prediction
def predict_churn(*args):
  #args are usedwhen v don't  know the no of arguments
  model,scaler,selected_features=load_model_component()
  input_data=pd.DataFrame([args],columns=selected_features)
  print(input_data)
  #scale the data
  input_scaled=scaler.transform(input_data)
  print(input_scaled)
  #predict
  prediction=model.predict(input_scaled)[0][0]

  probability=float(prediction)
  churn_status="Yes" if prediction>0.5 else "No"
  return {"churn Probability":probability*100,"will churn": churn_status}

predict_churn(2,0,40,50,2)